# -*- coding: utf-8 -*-
"""S24_004_Group_4_Project_1_Rudiâ€™s_Optimization_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aJ0muwUasBozEkuJ3UnGVG1X1YuBrC4c

**Rudi's Bakery Route Optimization: An Integrated Approach using Machine Learning and Optimization Modeling**

**Part I: Geocoding Store Delivery Locations and Generating Distance/Duration Matrices**

**Setting Up the Necessary Python Libraries**
"""

# This Script installs and imports the necessary libraries required for the optimization and data manipulation tasks in
# this notebook.

# `gurobipy`: A Python library for mathematical optimization using the Gurobi Optimizer.
# `pandas`: A library for data manipulation and analysis.
# `numpy`: A library for numerical computations.
# `requests`: A library for making HTTP requests in Python.
# `googlemaps`: A library for interacting with Google Maps API.

# Each one makes our data analysis and problem-solving tasks easier and more powerful

!pip install gurobipy
import gurobipy as gp

!pip install pandas
import pandas as pd

!pip install numpy
import numpy as np

!pip install requests
import requests

!pip install googlemaps
import googlemaps

"""**Geocoding Store Addresses and Extracting Coordinates**"""

# This script retrieves the latitude and longitude for a list of addresses from a CSV file using the Google Maps Geocoding API.
# It's designed to help organize delivery routes by converting physical addresses into geographical coordinates.

# Function to get the latitude and longitude coordinates for a given address using the Google Maps Geocoding API.
# param address: The address to geocode.
# param api_key: The API key for Google Maps services.
# return: A tuple of (latitude, longitude) if successful, None otherwise.

def get_geocode(address, api_key):
    base_url = "https://maps.googleapis.com/maps/api/geocode/json"
    params = {
        "address": address,
        "key": api_key
    }
    response = requests.get(base_url, params=params)
    if response.status_code == 200:
        result = response.json()
        if result["status"] == "OK":
            latitude = result["results"][0]["geometry"]["location"]["lat"]
            longitude = result["results"][0]["geometry"]["location"]["lng"]
            return latitude, longitude
    return None

# Update this path to match the location of your CSV file
# If the Jupyter Notebook and the CSV file are in the same directory, just use the file name
csv_file_path = 'DeliveryPoints.csv'

# Read CSV file
df = pd.read_csv(csv_file_path)

# Your Google Maps API key
api_key = "AIzaSyAJa7jnWhZkkn1EZew1lhOPIwGkejs7D6k"  # Make sure to replace this with your actual API key

# Extract relevant columns from the DataFrame
store_id = df['ID No'].tolist()
store = df['Store'].tolist()
addresses = df['Store Address'].tolist()

# Geocode each address and store the coordinates
coordinates = []
for address in addresses:
    lat_lng = get_geocode(address, api_key)
    if lat_lng:
        coordinates.append(lat_lng)
    else:
        print(f"Geocoding API failed for address: {address}")

"""**Archiving Store Information and Coordinates to a CSV File**"""

# This script takes our prepared lists of store IDs, names, addresses, and their corresponding geographical coordinates
# to create a new CSV file. This file organizes our delivery points efficiently, making it easier to visualize and
# manage locations for logistical planning.

import csv

# Assuming you have lists for store IDs, store names, addresses, and coordinates
store_id = store_id  # This should be your list of StoreIDs
store = store  # List of store names
addresses = addresses  # List of addresses
coordinates = coordinates  # List of coordinates (latitude, longitude)

# Combine the store IDs, store names, addresses, and coordinates into a single list of tuples
data_with_store_ids = list(zip(store_id, store, addresses, coordinates))

# Define the name of the CSV file to be created
csv_file_name = 'DeliveryPoints_coordinates_with_addresses.csv'

# Create and write the store IDs, store names, coordinates, and addresses to a CSV file
with open(csv_file_name, 'w', newline='') as file:
    writer = csv.writer(file)
    # Write the header
    writer.writerow(['Store_ID', 'Store', 'Store Address', 'Latitude', 'Longitude'])
    # Write the data
    for store_id, store_name, address, (lat, lng) in data_with_store_ids:
        writer.writerow([store_id, store_name, address, lat, lng])

# Print a confirmation message indicating that the file has been saved
# In a Jupyter Notebook, files are saved to the notebook's server file system
print(f"File saved: {csv_file_name}")

"""**Generating Integrated Distance and Duration Matrices for Delivery Points with Google Maps Directions API**"""

# This script employs Google Maps to calculate the distance and travel time between pairs of delivery points based on
# driving routes. It constructs two matrices: one for distances in miles and another for durations in minutes. These
# matrices provide a comprehensive view of the travel requirements between locations, crucial for optimizing delivery
# routes and scheduling.

# List of coordinates (latitude, longitude)
coordinates = coordinates

# Initialize the Google Maps client with your API key
gmaps = googlemaps.Client(key='AIzaSyAJa7jnWhZkkn1EZew1lhOPIwGkejs7D6k') # Make sure to replace this with your actual API key

# Determine the number of locations
num_locations = len(coordinates)

# Initialize matrices for distances and durations
distance_matrix = np.zeros((num_locations, num_locations))
duration_matrix = np.zeros((num_locations, num_locations))

# Calculate distances and durations between each pair of locations
for i in range(num_locations):
    for j in range(num_locations):
        if i == j:
            continue  # Skip distance to itself
        start = coordinates[i]
        end = coordinates[j]
        # Format coordinates as "lat,long" for the API
        start_str = f"{start[0]},{start[1]}"  # Corrected order to lat,long
        end_str = f"{end[0]},{end[1]}"

        # Request directions from Google Maps API
        directions_result = gmaps.directions(start_str,
                                             end_str,
                                             mode="driving",
                                             departure_time='now',
                                             traffic_model='best_guess')

        if directions_result:
            # Extract distance and duration from the API response
            distance = directions_result[0]['legs'][0]['distance']['value'] / 1609.34  # meters to miles
            duration = directions_result[0]['legs'][0]['duration']['value'] / 60  # seconds to minutes

            # Update the matrices with the calculated values
            distance_matrix[i][j] = distance
            duration_matrix[i][j] = duration

# Convert matrices to DataFrames for readability
distance_df = pd.DataFrame(distance_matrix, index=range(num_locations), columns=range(num_locations))
duration_df = pd.DataFrame(duration_matrix, index=range(num_locations), columns=range(num_locations))

# Print the resulting matrices
print("Distance Matrix (miles):")
print(distance_df)
print("\nDuration Matrix (minutes):")
print(duration_df)

"""**Exporting Store Information and Distance/Duration Matrices to an Excel File**"""

# This script transforms our lists of store details and their distances and durations between each other into structured
# tables. It then saves these tables as separate sheets within an Excel workbook, making it easy to manage and visualize
# our network of delivery points. Bold formatting is applied to highlight the Store_ID column, ensuring clarity and focus
# where it matters most.

# Install the xlsxwriter library for writing to Excel files
!pip install xlsxwriter

# Assuming you have lists for store IDs, store names, addresses, and coordinates
store_id = df['ID No'].tolist() # List of store IDs
store = store  # List of store names
addresses = addresses  # List of addresses
coordinates = coordinates  # List of coordinates (latitude, longitude)
distance_df = distance_df  # DataFrame containing distance matrix
duration_df = duration_df  # DataFrame containing duration matrix

# Convert the store information into a DataFrame
store_info_df = pd.DataFrame({
    'Store_ID': store_id,
    'Store': store,
    'Store Address': addresses,
    'Latitude': [lat for lat, lng in coordinates],
    'Longitude': [lng for lat, lng in coordinates]
})

# Ensure the Store_ID column is correctly set in distance_df and duration_df
distance_df['Store_ID'] = store_id  # This will create or overwrite the Store_ID column
duration_df['Store_ID'] = store_id  # This will create or overwrite the Store_ID column

# Reorder the columns to make Store_ID the first column if it's not already
distance_df = distance_df[['Store_ID'] + [col for col in distance_df.columns if col != 'Store_ID']]
duration_df = duration_df[['Store_ID'] + [col for col in duration_df.columns if col != 'Store_ID']]

# Specify the Excel file name
excel_file_name = 'DeliveryPoints_Matrix.xlsx'

# Using ExcelWriter to write DataFrames to separate sheets in the same Excel file
with pd.ExcelWriter(excel_file_name, engine='xlsxwriter') as writer:
    store_info_df.to_excel(writer, sheet_name='Store Information', index=False)
    distance_df.to_excel(writer, sheet_name='Distance', index=False)
    duration_df.to_excel(writer, sheet_name='Duration', index=False)

    # Apply formatting to the Excel file
    workbook = writer.book
    bold_format = workbook.add_format({'bold': True})

    # Apply bold format to the Store_ID column in all sheets
    for sheet_name in writer.sheets:
        worksheet = writer.sheets[sheet_name]
        worksheet.set_column('A:A', None, bold_format)

# Print a message indicating that the file has been saved
# In a Jupyter Notebook, files are saved to the notebook's server file system
print(f"File saved: {excel_file_name}")

"""**Part II: Cluster-Centric Store Delivery Points Segregation with K-Means Clustering Algorithm**

**Strategic Allocation of Store Delivery Points**
"""

# This script uses the K-Means clustering algorithm to organize delivery points into two groups based on the duration
# matrix from our Excel data. Initially, it forms two clusters to minimize travel time. Then, it balances these clusters
# to ensure each has a similar number of stores and total travel time is evenly distributed. The aim is to create efficient
# and equitable delivery routes, enhancing our logistics planning.

from sklearn.cluster import KMeans

# Load the Excel file containing the duration matrix
file_path = 'DeliveryPoints_Matrix.xlsx'
xls = pd.ExcelFile(file_path)
time_matrix_data = pd.read_excel(xls, sheet_name='Duration').drop("Store_ID", axis=1)

# Perform initial K-Means clustering to divide the delivery points into 2 clusters
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
kmeans.fit(time_matrix_data)
cluster_labels = kmeans.labels_

# Define a function to calculate the total time for each cluster
def calculate_total_time(matrix, labels):
    total_time_per_cluster = [np.sum(matrix[labels == i].sum(axis=1)) for i in range(2)]
    return total_time_per_cluster

# Define a function to balance clusters by both the number of stores and total time
def balance_clusters_equally(matrix, labels, depot_index=0, target_range=(24, 30), max_iterations=1000):
    for iteration in range(max_iterations):
        total_time_per_cluster = calculate_total_time(matrix, labels)
        stores_per_cluster = [sum(labels == i) for i in range(2)]

        # Check if the clusters are balanced within the target range and total time difference is acceptable
        if all(target_range[0] <= count <= target_range[1] for count in stores_per_cluster) and \
           abs(total_time_per_cluster[0] - total_time_per_cluster[1]) < np.mean(total_time_per_cluster) * 0.1:
            break

        # Determine which cluster to move a store from
        if not all(target_range[0] <= count <= target_range[1] for count in stores_per_cluster):
            from_cluster = np.argmax(stores_per_cluster)
        else:
            from_cluster = np.argmax(total_time_per_cluster)
        to_cluster = 1 - from_cluster

        # Find the best candidate store to move between clusters
        candidates = np.where((labels == from_cluster) & (np.arange(len(labels)) != depot_index))[0]
        if len(candidates) > 0:
            best_candidate, best_improvement = None, float('inf')
            for candidate in candidates:
                new_labels = labels.copy()
                new_labels[candidate] = to_cluster
                new_total_time_per_cluster = calculate_total_time(matrix, new_labels)
                new_stores_per_cluster = [sum(new_labels == i) for i in range(2)]
                time_diff = abs(new_total_time_per_cluster[0] - new_total_time_per_cluster[1])
                store_balance_improvement = abs((stores_per_cluster[0] - stores_per_cluster[1]) - (new_stores_per_cluster[0] - new_stores_per_cluster[1]))
                improvement_metric = time_diff + store_balance_improvement
                if improvement_metric < best_improvement:
                    best_candidate = candidate
                    best_improvement = improvement_metric
            if best_candidate is not None:
                labels[best_candidate] = to_cluster

    return labels

# Apply the balancing function to the initial cluster labels
balanced_labels_equally = balance_clusters_equally(time_matrix_data, cluster_labels.copy())

# Retrieve the original store IDs from the "Duration" sheet for final assignment
store_ids = pd.read_excel(xls, sheet_name='Duration')['Store_ID']
final_cluster_assignments = pd.DataFrame({
    'Store ID': store_ids,
    'Cluster': balanced_labels_equally
})

# Ensure depot is included in both clusters
final_cluster_1_ids = final_cluster_assignments[final_cluster_assignments['Cluster'] == 0]['Store ID'].tolist()
final_cluster_2_ids = final_cluster_assignments[final_cluster_assignments['Cluster'] == 1]['Store ID'].tolist()

# Check if the depot (ID=0) is in Cluster 1, if not, add it
if 0 not in final_cluster_1_ids:
    final_cluster_1_ids.append(0)

# Check if the depot (ID=0) is in Cluster 2, if not, add it
if 0 not in final_cluster_2_ids:
    final_cluster_2_ids.append(0)

# Sort the IDs in each cluster for better readability
final_cluster_1_ids_sorted = sorted(final_cluster_1_ids)
final_cluster_2_ids_sorted = sorted(final_cluster_2_ids)

# Print the sorted lists of store IDs for each cluster, including the depot
print("Cluster 1 IDs including depot:", final_cluster_1_ids_sorted)
print("Cluster 2 IDs including depot:", final_cluster_2_ids_sorted)

"""**Exporting Route 1 Cluster-Specific Delivery Points Data to Excel**"""

# This script fine-tunes our treasure trove of delivery points by selecting only those within a specified cluster, thereby
# creating a focused dataset. It filters both distance and duration matrices, along with store information, to include only
# the relevant delivery points from the first cluster. The refined data is then saved into a new Excel workbook, streamlining
# our route planning for more efficient deliveries.

# Install the xlsxwriter library for writing to Excel files
!pip install xlsxwriter

import pandas as pd

# Load the Excel file containing delivery points matrices and store information
# In a Jupyter Notebook, files are saved to the notebook's server file system
file_path = 'DeliveryPoints_Matrix.xlsx'
xls = pd.ExcelFile(file_path)

# Load the data sheets from the Excel file into a dictionary
data_sheets = {sheet_name: pd.read_excel(xls, sheet_name=sheet_name) for sheet_name in xls.sheet_names}

# Define the store IDs for the first cluster
cluster1_store_ids = final_cluster_1_ids_sorted

# Function to filter the Distance and Duration matrices for a specific cluster
def filter_matrices_correctly(matrix_df, cluster_ids):
    # Ensure the Store_ID column is in the correct format
    matrix_df.set_index('Store_ID', inplace=True)
    # Keep only rows and columns corresponding to the cluster IDs
    filtered_matrix = matrix_df.loc[cluster_ids, cluster_ids]
    # Reset index to bring Store_ID back as a column
    filtered_matrix.reset_index(inplace=True)
    return filtered_matrix

# Apply the filtering function to the Distance and Duration matrices for the first cluster
filtered_distance_df = filter_matrices_correctly(data_sheets['Distance'], cluster1_store_ids)
filtered_duration_df = filter_matrices_correctly(data_sheets['Duration'], cluster1_store_ids)

# Filter the Store Information sheet to include only the stores in the first cluster
filtered_store_info_df = data_sheets['Store Information'][data_sheets['Store Information']['Store_ID'].isin(cluster1_store_ids)]

# Save the filtered data for the first cluster to a new Excel file
output_file_path = 'Route1_DeliveryPoints_Matrix.xlsx'  # Adjust the path if necessary
with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:
    filtered_store_info_df.to_excel(writer, sheet_name='Store Information', index=False)
    filtered_distance_df.to_excel(writer, sheet_name='Distance', index=False)
    filtered_duration_df.to_excel(writer, sheet_name='Duration', index=False)

# Print a confirmation message indicating the file has been saved
# In a Jupyter Notebook, files are saved to the notebook's server file system
print("Filtered Excel file has been saved:", output_file_path)

"""**Exporting Route 2 Cluster-Specific Delivery Points Data to Excel**"""

# This script fine-tunes our treasure trove of delivery points by selecting only those within a specified cluster, thereby
# creating a focused dataset. It filters both distance and duration matrices, along with store information, to include only
# the relevant delivery points from the first cluster. The refined data is then saved into a new Excel workbook, streamlining
# our route planning for more efficient deliveries.

# Install the xlsxwriter library for writing to Excel files
!pip install xlsxwriter

import pandas as pd

# Load the Excel file containing delivery points matrices and store information
# In a Jupyter Notebook, files are saved to the notebook's server file system
file_path = 'DeliveryPoints_Matrix.xlsx'  # Adjust the path if necessary
xls = pd.ExcelFile(file_path)

# Load the data sheets from the Excel file into a dictionary
data_sheets = {sheet_name: pd.read_excel(xls, sheet_name=sheet_name) for sheet_name in xls.sheet_names}

# Define the store IDs for the second cluster
cluster2_store_ids = final_cluster_2_ids_sorted

# Function to filter the Distance and Duration matrices for a specific cluster
def filter_matrices_correctly(matrix_df, cluster_ids):
    # Ensure the Store_ID column is in the correct format
    matrix_df.set_index('Store_ID', inplace=True)
    # Keep only rows and columns corresponding to the cluster IDs
    filtered_matrix = matrix_df.loc[cluster_ids, cluster_ids]
    # Reset index to bring Store_ID back as a column
    filtered_matrix.reset_index(inplace=True)
    return filtered_matrix

# Apply the filtering function to the Distance and Duration matrices for the second cluster
filtered_distance_df = filter_matrices_correctly(data_sheets['Distance'], cluster2_store_ids)
filtered_duration_df = filter_matrices_correctly(data_sheets['Duration'], cluster2_store_ids)

# Filter the Store Information sheet to include only the stores in the second cluster
filtered_store_info_df = data_sheets['Store Information'][data_sheets['Store Information']['Store_ID'].isin(cluster2_store_ids)]

# Save the filtered data for the second cluster to a new Excel file
output_file_path = 'Route2_DeliveryPoints_Matrix.xlsx'  # Adjust the path if necessary
with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:
    filtered_store_info_df.to_excel(writer, sheet_name='Store Information', index=False)
    filtered_distance_df.to_excel(writer, sheet_name='Distance', index=False)
    filtered_duration_df.to_excel(writer, sheet_name='Duration', index=False)

# Print a confirmation message indicating the file has been saved
# In a Jupyter Notebook, files are saved to the notebook's server file system
print("Filtered Excel file has been saved:", output_file_path)

"""**Part IIIA: Route 1 Delivery Points Optimization using Gurobi Solver**

**Data Preparation for Route 1 Delivery Points Dataset**
"""

# This script sets the stage for optimizing delivery routes by loading store details, along with distance and duration
# matrices, from an Excel workbook. It transforms this information into dictionaries, mapping each pair of origin and
# destination to their respective travel times and distances. Additionally, it outlines the process of identifying
# specific delivery days, particularly for stores like Costco, and prepares a foundation for calculating loading times
# and working hours, crucial for the subsequent steps in route planning.

# Adjust the file path below
file_path = 'Route1_DeliveryPoints_Matrix.xlsx'  # Update this path

from gurobipy import Model, GRB, quicksum

# Load the Excel file using the correct sheet names
store_details = pd.read_excel(file_path, sheet_name='Store Information')
time_matrix_df = pd.read_excel(file_path, sheet_name='Duration')
distance_matrix_df = pd.read_excel(file_path, sheet_name='Distance')

# Convert the Duration DataFrame into a dictionary where keys are (origin, destination) pairs and values are travel times
time_matrix = {
    (int(time_matrix_df.iloc[i]['Store_ID']), int(time_matrix_df.columns[j])): time_matrix_df.iloc[i, j]
    for i in range(len(time_matrix_df))
    for j in range(1, len(time_matrix_df.columns))
}

# Convert the Distance DataFrame into a dictionary where keys are (origin, destination) pairs and values are travel times
distance_matrix = {
    (int(distance_matrix_df.iloc[i]['Store_ID']), int(distance_matrix_df.columns[j])): distance_matrix_df.iloc[i, j]
    for i in range(len(distance_matrix_df))
    for j in range(1, len(distance_matrix_df.columns))
}

# Process 'store_details' to identify delivery days for each store
# Initialize a dictionary to keep track of delivery days for Costco and other stores
delivery_days = {}

# Define working days, excluding Wednesday and Sunday
working_days = ['Monday', 'Tuesday', 'Thursday', 'Friday', 'Saturday']

# Prepare a matrix for loading times (assuming 15 minutes for each store)
loading_time = 15  # Loading time in minutes
working_hours_per_day = 9 * 60

# Extract store IDs, including the depot (assumed to be ID 0)
stores = sorted(store_details['Store_ID'].unique().tolist())

# Identify Costco stores from the 'Store' column
costco_stores = store_details[store_details['Store'].str.contains('Costco')]['Store_ID'].unique().tolist()

"""**Defining Decision Variables, Objective Function, and Constraints (Route 1)**"""

# In this script, we conjure an optimization model to determine the most efficient delivery routes. By defining binary decision
# variables for visits and deliveries, we set our sights on minimizing total travel time. Our primary objective is to pursue
# the minimization of total travel time. Constraints ensure each journey begins and ends at the depot, maintains path
# continuity, and meets delivery requirementsâ€”special attention is given to Costco's daily delivery needs. We also heed
# the limits on daily deliveries and working hours to keep our routes practical and manageable. The goal is to weave through
# our delivery points with precision, balancing efficiency with operational constraints.

# Initialize the model
model = Model("VehicleRouteOptimization")

# Decision Variables
# Create binary variables to represent if a path between two stores is visited on a particular day
visit_var = model.addVars(
    working_days, [(i, j) for i, j in time_matrix.keys() if i != j],
    vtype=GRB.BINARY, name="visit"
)

# Create binary variables to represent if a store receives a delivery on a particular day
delivery_var = model.addVars(
    working_days, store_details['Store_ID'].unique(),
    vtype=GRB.BINARY, name="delivery"
)

# Objective Function: Minimize the total travel time across all days and path
model.setObjective(
    quicksum(visit_var[day, i, j] * time_matrix[(i, j)]
             for day in working_days for i, j in time_matrix.keys() if i != j),
    GRB.MINIMIZE
)

# Constraints
# Assuming the depot is represented as store 0
for day in working_days:
    # Ensure each route starts from the depot
    model.addConstr(quicksum(visit_var[day, 0, j] for j in stores if j != 0) == 1, f"StartFromDepot_{day}")
    # Ensure each route ends at the depot
    model.addConstr(quicksum(visit_var[day, i, 0] for i in stores if i != 0) == 1, f"ReturnToDepotLast_{day}")

# Ensure each store is visited exactly once per day if it receives a delivery
# i.e. Single exit and entry for each store, ensuring path continuity
for day in working_days:
    for store in stores[1:]:  # Excluding the depot
        model.addConstr(quicksum(visit_var[day, i, store] for i in stores if i != store) >= delivery_var[day, store], f"MinIncoming_{day}_{store}")
        model.addConstr(quicksum(visit_var[day, store, j] for j in stores if j != store) >= delivery_var[day, store], f"MinOutgoing_{day}_{store}")

# Ensure Costco stores receive daily deliveries and other stores receive at least 3 deliveries per week
for store in stores[1:]:
    if store in costco_stores:
        for day in working_days:
            model.addConstr(delivery_var[day, store] == 1, f"CostcoDelivery_{day}_{store}")
    else:
        model.addConstr(quicksum(delivery_var[day, store] for day in working_days) >= 3, f"MinDeliveries_{store}")

# Flexible constraint to avoid deliveries after two consecutive delivery days
for store in stores:
    for i in range(len(working_days) - 2):
        day1, day2, day3 = working_days[i], working_days[i + 1], working_days[i + 2]
        model.addConstr((delivery_var[day1, store] + delivery_var[day2, store] + (1 - delivery_var[day3, store])) >= 1,
                        f"FlexibleNoConsecutiveDelivery_{store}_{day1}_{day2}_{day3}")

# Limit the maximum number of deliveries per day to ensure logistics feasibility
# i.e. the number of deliveries per day to less than 19
for day in working_days:
    model.addConstr(
        quicksum(delivery_var[day, store] for store in stores) <=18,
        f"MaxDeliveriesPerDay_{day}"
    )

# Ensure the total working hours per day are not exceeded, accounting for travel and loading times
for day in working_days:
    model.addConstr(
        quicksum((time_matrix[(i, j)] + loading_time) * visit_var[day, i, j] for i in stores for j in stores if i != j) <= working_hours_per_day,
        f"WorkingHours_{day}"
    )

"""**Executing and Analyzing the Optimization Model Results (Route 1)**"""

# This Script solves the optimization model to determine daily routes, then sequences and formats the store visits
# for better readability.

# Execute the optimization process on the model to find the best routes
model.optimize()

# Initialize a dictionary to hold the sequence of store visits for each day
daily_visits = {day: [] for day in working_days}

# Loop through each day to extract and store the sequence of visits from the model's solution
for day in working_days:
    for i, j in [(i, j) for i in stores for j in stores if i != j]:
        # Check if a path between stores i and j is chosen by the model
        if visit_var[day, i, j].X > 0.5:  # Using a threshold to determine selection
            # Add the stores to the daily visit list if they are part of the route
            if i not in daily_visits[day]:
                daily_visits[day].append(i)
            if j not in daily_visits[day]:
                daily_visits[day].append(j)

# Define a function to sequence and format the store visits into a readable format
def format_sequence(sequence):
    # Append the depot as the final destination if not already present
    if sequence[-1] != 0:
        sequence.append(0)

    # Construct a string that narrates the route from store to store
    transitions = [f"store {sequence[i]} to store {sequence[i+1]}" for i in range(len(sequence) - 1)]
    formatted_sequence = "; ".join(transitions)
    return formatted_sequence

# Apply the formatting function to the daily visits for presentation
for day, stores in daily_visits.items():
    formatted_sequence = format_sequence(stores)
    print(f"Day: {day}, Route: {formatted_sequence}")

"""**Calculating Weekly Total Time for Optimized Delivery Routes (Route 1)**"""

# This Script calculates daily and weekly total delivery times in hours, summarizing the efficiency of the optimized routes.

# Initialize the total time counter for the entire week to zero
weekly_total_time = 0

# Define a function to compute the total operational time for a day's delivery route, including travel and loading times
def calculate_total_time_for_day(routes, time_matrix, loading_time):
    # Initialize the total time for a single day's route to zero
    total_time = 0
     # Iterate over each pair of consecutive stores in the route
    for i in range(len(routes) - 1):
        # Accumulate the travel time between each pair and add loading time
        total_time += time_matrix[(routes[i], routes[i+1])] + loading_time
    # Subtract the final loading time at the last store since it's not needed
    total_time = total_time - 15
    # Convert the total time from minutes to hours for better readability
    return total_time / 60  # Convert minutes to hours

# Loop through each day's route to calculate and display the total time required
for day, route in daily_visits.items():
    # Calculate the total time for the day using the predefined function
    total_time = calculate_total_time_for_day(route, time_matrix, loading_time)
    # Print the total time for the day in hours
    print(f"Day: {day}, Total Time: {total_time} hours")
    # Add the day's total time to the weekly total
    weekly_total_time += total_time  # Add day's total time to weekly total

# Display the total time spent on deliveries for the week
print(f"Total time for the week: {weekly_total_time} hours")

"""**Calculating Weekly Total Distance covered for Optimized Delivery Routes (Route 1)**"""

# This Script computes and presents the total distances traveled each day and for the entire week, offering insights
# into route length efficiency.

# Initialize the accumulator for the total distance covered in the entire week
weekly_total_distance = 0

# Define a function to compute the total distance covered for a day's delivery route
def calculate_total_distance_for_day(routes, distance_matrix):
    total_distance = 0
    for i in range(len(routes) - 1):
        # Sum the distances between consecutive locations based on the provided distance matrix
        total_distance += distance_matrix[(routes[i], routes[i+1])]
    return total_distance  # Returns the total distance in the same units as specified in the distance_matrix

# Loop through each day's routes to calculate and display the total distance traveled
for day, route in daily_visits.items():
    total_distance = calculate_total_distance_for_day(route, distance_matrix)
    # Display the total distance for the day, assuming miles as the unit of measurement
    print(f"Day: {day}, Total Distance: {total_distance} miles")
    # Increment the weekly total distance by the day's total
    weekly_total_distance += total_distance  # Add day's total distance to weekly total

# Display the total distance covered over the week
print(f"Total distance for the week: {weekly_total_distance} miles")  # units depend on how distance is measured in your matrix

"""**Generating and Exporting a Weekly Store Visit Schedule to Excel (Route 1)**"""

# This Script transforms optimized route decisions into a clear weekly schedule, marking store visits, and saves it as
# an Excel workbook.

import pandas as pd

# Initialize a DataFrame to structure the weekly delivery schedule for visualization
# The columns are all store IDs except 0, and the rows are the working days
schedule_df = pd.DataFrame(index=working_days, columns=[store for store in stores if store != 0]).fillna('')  # Start with an empty DataFrame

# Iterate over the 'daily_visits' dictionary to populate the DataFrame
for day in daily_visits:
    for store in daily_visits[day]:
        if store != 0:  # Depot is not considered in the schedule
            schedule_df.at[day, store] = 'Yes' # Mark 'Yes' for stores visited on each day

# Export the populated DataFrame to an Excel file for easy sharing and analysis
output_excel_path = 'Route1_Weekly_Store_Visits_Schedule.xlsx'
schedule_df.to_excel(output_excel_path)

# Print confirmation message indicating successful export
# In a Jupyter Notebook, files are saved to the notebook's server file system
print(f"Schedule saved to {output_excel_path}")

"""**Compiling and Exporting Detailed Daily Journey Metrics to Excel (Route 1)**"""

# This Script compiles detailed journey segments from optimized routes into a DataFrame and archives
# it in an Excel file for review.

import pandas as pd

# Preparing a structured list to capture detailed segments of each journey
journey_details = []

# Define a fixed loading time for operations at each store
loading_time_per_store = 15  # Loading time in minutes for each stop

# Loop through the optimized routes for each day to detail journey segments
for day in daily_visits:
    route = daily_visits[day]  # Retrieve the day's optimized route
    for i in range(len(route) - 1):
        from_store = route[i]
        to_store = route[i + 1]
        # Resolve the name of the destination store, or label the depot accordingly
        store_name = store_details.loc[store_details['Store_ID'] == to_store, 'Store'].values[0] if to_store != 0 else "Rudi's Depot"
        travel_time = time_matrix.get((from_store, to_store), 0) # Retrieve the travel time from the matrix
        distance = distance_matrix.get((from_store, to_store), 0) # Retrieve the distance from the matrix
        # Conditional loading time assignment based on destination
        loading_time = 0 if to_store == 0 else loading_time_per_store
         # Compute the total time spent in transit including loading times
        total_transit_time = travel_time + loading_time

        # Compile each journey segment into the details list
        journey_details.append([day, from_store, to_store, store_name, distance, travel_time, loading_time, total_transit_time])

# Transform the journey details list to a structured DataFrame
columns = ['Day', 'From Store_ID', 'To Store_ID', 'Store of To Store_ID', 'Distance Travelled (miles)', 'Time Taken to Travel (min)', 'Loading Time (min)', 'Total Transit Time (min)']
journey_df = pd.DataFrame(journey_details, columns=columns)

# Export the detailed journey metrics to an Excel file for comprehensive analysis
output_excel_path = 'Route1_Daily_Journey_Details.xlsx'
journey_df.to_excel(output_excel_path, index=False)

# Print the successful export of journey metrics
print(f"Journey details saved to {output_excel_path}")

"""**Part IIIB: Route 2 Delivery Points Optimization using Gurobi Solver**

**Data Preparation for Route 2 Delivery Points Dataset**
"""

# This script sets the stage for optimizing delivery routes by loading store details, along with distance and duration
# matrices, from an Excel workbook. It transforms this information into dictionaries, mapping each pair of origin and
# destination to their respective travel times and distances. Additionally, it outlines the process of identifying
# specific delivery days, particularly for stores like Costco, and prepares a foundation for calculating loading times
# and working hours, crucial for the subsequent steps in route planning.

# Adjust the file path below
file_path = 'Route2_DeliveryPoints_Matrix.xlsx'  # Update this path

from gurobipy import Model, GRB, quicksum

# Load the Excel file using the correct sheet names
store_details = pd.read_excel(file_path, sheet_name='Store Information')
time_matrix_df = pd.read_excel(file_path, sheet_name='Duration')
distance_matrix_df = pd.read_excel(file_path, sheet_name='Distance')

# Convert the Duration DataFrame into a dictionary where keys are (origin, destination) pairs and values are travel times
time_matrix = {
    (int(time_matrix_df.iloc[i]['Store_ID']), int(time_matrix_df.columns[j])): time_matrix_df.iloc[i, j]
    for i in range(len(time_matrix_df))
    for j in range(1, len(time_matrix_df.columns))
}

# Convert the Distance DataFrame into a dictionary where keys are (origin, destination) pairs and values are travel times
distance_matrix = {
    (int(distance_matrix_df.iloc[i]['Store_ID']), int(distance_matrix_df.columns[j])): distance_matrix_df.iloc[i, j]
    for i in range(len(distance_matrix_df))
    for j in range(1, len(distance_matrix_df.columns))
}

# Process 'store_details' to identify delivery days for each store
# Initialize a dictionary to keep track of delivery days for Costco and other stores
delivery_days = {}

# Define working days, excluding Wednesday and Sunday
working_days = ['Monday', 'Tuesday', 'Thursday', 'Friday', 'Saturday']

# Prepare a matrix for loading times (assuming 15 minutes for each store)
loading_time = 15  # Loading time in minutes
working_hours_per_day = 9 * 60

# Extract store IDs, including the depot (assumed to be ID 0)
stores = sorted(store_details['Store_ID'].unique().tolist())

# Identify Costco stores from the 'Store' column
costco_stores = store_details[store_details['Store'].str.contains('Costco')]['Store_ID'].unique().tolist()

"""**Defining Decision Variables, Objective Function, and Constraints (Route 2)**"""

# In this script, we conjure an optimization model to determine the most efficient delivery routes. By defining binary decision
# variables for visits and deliveries, we set our sights on minimizing total travel time. Our primary objective is to pursue
# the minimization of total travel time. Constraints ensure each journey begins and ends at the depot, maintains path
# continuity, and meets delivery requirementsâ€”special attention is given to Costco's daily delivery needs. We also heed
# the limits on daily deliveries and working hours to keep our routes practical and manageable. The goal is to weave through
# our delivery points with precision, balancing efficiency with operational constraints.

# Initialize the model
model = Model("VehicleRouteOptimization")

# Decision Variables
# Create binary variables to represent if a path between two stores is visited on a particular day
visit_var = model.addVars(
    working_days, [(i, j) for i, j in time_matrix.keys() if i != j],
    vtype=GRB.BINARY, name="visit"
)

# Create binary variables to represent if a store receives a delivery on a particular day
delivery_var = model.addVars(
    working_days, store_details['Store_ID'].unique(),
    vtype=GRB.BINARY, name="delivery"
)

# Objective Function: Minimize the total travel time across all days and path
model.setObjective(
    quicksum(visit_var[day, i, j] * time_matrix[(i, j)]
             for day in working_days for i, j in time_matrix.keys() if i != j),
    GRB.MINIMIZE
)

# Constraints
# Assuming the depot is represented as store 0
for day in working_days:
    # Ensure each route starts from the depot
    model.addConstr(quicksum(visit_var[day, 0, j] for j in stores if j != 0) == 1, f"StartFromDepot_{day}")
    # Ensure each route ends at the depot
    model.addConstr(quicksum(visit_var[day, i, 0] for i in stores if i != 0) == 1, f"ReturnToDepotLast_{day}")

# Ensure each store is visited exactly once per day if it receives a delivery
# i.e. Single exit and entry for each store, ensuring path continuity
for day in working_days:
    for store in stores[1:]:  # Excluding the depot
        model.addConstr(quicksum(visit_var[day, i, store] for i in stores if i != store) >= delivery_var[day, store], f"MinIncoming_{day}_{store}")
        model.addConstr(quicksum(visit_var[day, store, j] for j in stores if j != store) >= delivery_var[day, store], f"MinOutgoing_{day}_{store}")

# Ensure Costco stores receive daily deliveries and other stores receive at least 3 deliveries per week
for store in stores[1:]:
    if store in costco_stores:
        for day in working_days:
            model.addConstr(delivery_var[day, store] == 1, f"CostcoDelivery_{day}_{store}")
    else:
        model.addConstr(quicksum(delivery_var[day, store] for day in working_days) >= 3, f"MinDeliveries_{store}")

# Flexible constraint to avoid deliveries after two consecutive delivery days
for store in stores:
    for i in range(len(working_days) - 2):
        day1, day2, day3 = working_days[i], working_days[i + 1], working_days[i + 2]
        model.addConstr((delivery_var[day1, store] + delivery_var[day2, store] + (1 - delivery_var[day3, store])) >= 1,
                        f"FlexibleNoConsecutiveDelivery_{store}_{day1}_{day2}_{day3}")

# Limit the maximum number of deliveries per day to ensure logistics feasibility
# i.e. the number of deliveries per day to less than 17
for day in working_days:
    model.addConstr(
        quicksum(delivery_var[day, store] for store in stores) <=16,
        f"MaxDeliveriesPerDay_{day}"
    )

# Ensure the total working hours per day are not exceeded, accounting for travel and loading times
for day in working_days:
    model.addConstr(
        quicksum((time_matrix[(i, j)] + loading_time) * visit_var[day, i, j] for i in stores for j in stores if i != j) <= working_hours_per_day,
        f"WorkingHours_{day}"
    )

"""**Executing and Analyzing the Optimization Model Results (Route 2)**"""

# This Script solves the optimization model to determine daily routes, then sequences and formats the store visits
# for better readability.

# Execute the optimization process on the model to find the best routes
model.optimize()

# Initialize a dictionary to hold the sequence of store visits for each day
daily_visits = {day: [] for day in working_days}

# Loop through each day to extract and store the sequence of visits from the model's solution
for day in working_days:
    for i, j in [(i, j) for i in stores for j in stores if i != j]:
        # Check if a path between stores i and j is chosen by the model
        if visit_var[day, i, j].X > 0.5:  # Using a threshold to determine selection
            # Add the stores to the daily visit list if they are part of the route
            if i not in daily_visits[day]:
                daily_visits[day].append(i)
            if j not in daily_visits[day]:
                daily_visits[day].append(j)

# Define a function to sequence and format the store visits into a readable format
def format_sequence(sequence):
    # Append the depot as the final destination if not already present
    if sequence[-1] != 0:
        sequence.append(0)

    # Construct a string that narrates the route from store to store
    transitions = [f"store {sequence[i]} to store {sequence[i+1]}" for i in range(len(sequence) - 1)]
    formatted_sequence = "; ".join(transitions)
    return formatted_sequence

# Apply the formatting function to the daily visits for presentation
for day, stores in daily_visits.items():
    formatted_sequence = format_sequence(stores)
    print(f"Day: {day}, Route: {formatted_sequence}")

"""**Calculating Weekly Total Time for Optimized Delivery Routes (Route 2)**"""

# This Script calculates daily and weekly total delivery times in hours, summarizing the efficiency of the optimized routes.

# Initialize the total time counter for the entire week to zero
weekly_total_time = 0

# Define a function to compute the total operational time for a day's delivery route, including travel and loading times
def calculate_total_time_for_day(routes, time_matrix, loading_time):
    # Initialize the total time for a single day's route to zero
    total_time = 0
     # Iterate over each pair of consecutive stores in the route
    for i in range(len(routes) - 1):
        # Accumulate the travel time between each pair and add loading time
        total_time += time_matrix[(routes[i], routes[i+1])] + loading_time
    # Subtract the final loading time at the last store since it's not needed
    total_time = total_time - 15
    # Convert the total time from minutes to hours for better readability
    return total_time / 60  # Convert minutes to hours

# Loop through each day's route to calculate and display the total time required
for day, route in daily_visits.items():
    # Calculate the total time for the day using the predefined function
    total_time = calculate_total_time_for_day(route, time_matrix, loading_time)
    # Print the total time for the day in hours
    print(f"Day: {day}, Total Time: {total_time} hours")
    # Add the day's total time to the weekly total
    weekly_total_time += total_time  # Add day's total time to weekly total

# Display the total time spent on deliveries for the week
print(f"Total time for the week: {weekly_total_time} hours")

"""**Calculating Weekly Total Distance covered for Optimized Delivery Routes (Route 2)**"""

# This Script computes and presents the total distances traveled each day and for the entire week, offering insights
# into route length efficiency.

# Initialize the accumulator for the total distance covered in the entire week
weekly_total_distance = 0

# Define a function to compute the total distance covered for a day's delivery route
def calculate_total_distance_for_day(routes, distance_matrix):
    total_distance = 0
    for i in range(len(routes) - 1):
        # Sum the distances between consecutive locations based on the provided distance matrix
        total_distance += distance_matrix[(routes[i], routes[i+1])]
    return total_distance  # Returns the total distance in the same units as specified in the distance_matrix

# Loop through each day's routes to calculate and display the total distance traveled
for day, route in daily_visits.items():
    total_distance = calculate_total_distance_for_day(route, distance_matrix)
    # Display the total distance for the day, assuming miles as the unit of measurement
    print(f"Day: {day}, Total Distance: {total_distance} miles")
    # Increment the weekly total distance by the day's total
    weekly_total_distance += total_distance  # Add day's total distance to weekly total

# Display the total distance covered over the week
print(f"Total distance for the week: {weekly_total_distance} miles")  # units depend on how distance is measured in your matrix

"""**Generating and Exporting a Weekly Store Visit Schedule to Excel (Route 2)**"""

# This Script transforms optimized route decisions into a clear weekly schedule, marking store visits, and saves it as
# an Excel workbook.

import pandas as pd

# Initialize a DataFrame to structure the weekly delivery schedule for visualization
# The columns are all store IDs except 0, and the rows are the working days
schedule_df = pd.DataFrame(index=working_days, columns=[store for store in stores if store != 0]).fillna('')  # Start with an empty DataFrame

# Iterate over the 'daily_visits' dictionary to populate the DataFrame
for day in daily_visits:
    for store in daily_visits[day]:
        if store != 0:  # Depot is not considered in the schedule
            schedule_df.at[day, store] = 'Yes' # Mark 'Yes' for stores visited on each day

# Export the populated DataFrame to an Excel file for easy sharing and analysis
output_excel_path = 'Route2_Weekly_Store_Visits_Schedule.xlsx'
schedule_df.to_excel(output_excel_path)

# Print confirmation message indicating successful export
# In a Jupyter Notebook, files are saved to the notebook's server file system
print(f"Schedule saved to {output_excel_path}")

"""**Compiling and Exporting Detailed Daily Journey Metrics to Excel (Route 2)**"""

# This Script compiles detailed journey segments from optimized routes into a DataFrame and archives
# it in an Excel file for review.

import pandas as pd

# Preparing a structured list to capture detailed segments of each journey
journey_details = []

# Define a fixed loading time for operations at each store
loading_time_per_store = 15  # Loading time in minutes for each stop

# Loop through the optimized routes for each day to detail journey segments
for day in daily_visits:
    route = daily_visits[day]  # Retrieve the day's optimized route
    for i in range(len(route) - 1):
        from_store = route[i]
        to_store = route[i + 1]
        # Resolve the name of the destination store, or label the depot accordingly
        store_name = store_details.loc[store_details['Store_ID'] == to_store, 'Store'].values[0] if to_store != 0 else "Rudi's Depot"
        travel_time = time_matrix.get((from_store, to_store), 0) # Retrieve the travel time from the matrix
        distance = distance_matrix.get((from_store, to_store), 0) # Retrieve the distance from the matrix
        # Conditional loading time assignment based on destination
        loading_time = 0 if to_store == 0 else loading_time_per_store
         # Compute the total time spent in transit including loading times
        total_transit_time = travel_time + loading_time

        # Compile each journey segment into the details list
        journey_details.append([day, from_store, to_store, store_name, distance, travel_time, loading_time, total_transit_time])

# Transform the journey details list to a structured DataFrame
columns = ['Day', 'From Store_ID', 'To Store_ID', 'Store of To Store_ID', 'Distance Travelled (miles)', 'Time Taken to Travel (min)', 'Loading Time (min)', 'Total Transit Time (min)']
journey_df = pd.DataFrame(journey_details, columns=columns)

# Export the detailed journey metrics to an Excel file for comprehensive analysis
output_excel_path = 'Route2_Daily_Journey_Details.xlsx'
journey_df.to_excel(output_excel_path, index=False)

# Print the successful export of journey metrics
print(f"Journey details saved to {output_excel_path}")
